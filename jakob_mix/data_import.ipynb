{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Importing data generator and training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.getcwd())\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import data_gen1\n",
    "\n",
    "np.random.seed(69)\n",
    "# torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[[-1.17437157, -0.10885711],\n         [ 3.50118478,  3.87670562]],\n \n        [[-1.36466648, -0.54565739],\n         [ 3.45002393,  3.8294597 ]]]),\n array([0.5450571 , 0.77598356]))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_data(num_points, point_size=1000, min_var0=0.1, max_var0=10, min_var1=0.1, max_var1=10, dim=1):\n",
    "    x0_var = np.random.uniform(min_var0, max_var0, num_points)\n",
    "    x1_var = np.random.uniform(min_var1, max_var1, num_points)\n",
    "    data = np.zeros([num_points, point_size, 2, dim])\n",
    "    label = np.zeros([num_points])\n",
    "\n",
    "    for i in range(num_points):\n",
    "        data[i], label[i] = data_gen1.gen_list(\n",
    "            point_size, x0_var[i], x1_var[i], dim)\n",
    "\n",
    "    return data, label\n",
    "\n",
    "def create_data2(realisations, xy_len=1000, min_var0=0.1, max_var0=10, min_var1=0.1, max_var1=10, dim=1):\n",
    "    x0_var = np.random.uniform(min_var0, max_var0, realisations)\n",
    "    x1_var = np.random.uniform(min_var1, max_var1, realisations)\n",
    "    data = np.zeros((realisations, xy_len, 2))\n",
    "    label = np.zeros([realisations])\n",
    "\n",
    "    for i in range(realisations):\n",
    "        _test, label[i] = data_gen1.gen_list(\n",
    "            xy_len, x0_var[i], x1_var[i], dim)\n",
    "        x, y = _test[:, 0, :], _test[:, 1, :]\n",
    "\n",
    "        data[i] = np.hstack((x, y))\n",
    "\n",
    "    return data, label\n",
    "\n",
    "data, label = create_data2(2, 2)\n",
    "data, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Test data import\n",
    "class Article_nn(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Article_nn, self).__init__()\n",
    "        self.l1 = nn.Linear(2, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(64, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.sig(self.l2(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class NumbersDataset(Dataset):\n",
    "    def __init__(self, num_points, point_size):\n",
    "        self.samples, self.labels = create_data2(num_points, point_size)\n",
    "        self.samples = torch.from_numpy(self.samples).to(torch.float32)\n",
    "        self.labels = torch.from_numpy(self.labels).to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples, self.labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(train_loader, learning_rate, num_epoch, input_size):\n",
    "    model = Article_nn(input_size)\n",
    "    # print(train_loader)\n",
    "    # loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # training loop\n",
    "    n_total_steps = len(train_loader)\n",
    "    for epoch in range(num_epoch):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            sample = images.to(device)\n",
    "            #labels = torch.ones(\n",
    "            #    (len(sample), 1)) * labels.item()  # reshaping because labels needs to be same size as output\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            print(sample.shape, labels, labels.shape)\n",
    "            # forward\n",
    "            output = model(sample)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i) % 1 == 0:\n",
    "                print(\n",
    "                    f'epoch {epoch} / {num_epoch}, step {i}/{n_total_steps} loss = {loss.item():.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 20, 100, 2]) tensor([[0.1745, 0.2147, 0.3821, 0.8402, 0.1906, 0.6002, 0.3752, 0.6744, 0.1280,\n",
      "         0.1355, 0.9050, 1.4088, 0.2229, 0.7396, 1.6503, 0.7367, 1.0233, 1.6512,\n",
      "         0.5259, 0.0636],\n",
      "        [0.1745, 0.2147, 0.3821, 0.8402, 0.1906, 0.6002, 0.3752, 0.6744, 0.1280,\n",
      "         0.1355, 0.9050, 1.4088, 0.2229, 0.7396, 1.6503, 0.7367, 1.0233, 1.6512,\n",
      "         0.5259, 0.0636],\n",
      "        [0.1745, 0.2147, 0.3821, 0.8402, 0.1906, 0.6002, 0.3752, 0.6744, 0.1280,\n",
      "         0.1355, 0.9050, 1.4088, 0.2229, 0.7396, 1.6503, 0.7367, 1.0233, 1.6512,\n",
      "         0.5259, 0.0636],\n",
      "        [0.1745, 0.2147, 0.3821, 0.8402, 0.1906, 0.6002, 0.3752, 0.6744, 0.1280,\n",
      "         0.1355, 0.9050, 1.4088, 0.2229, 0.7396, 1.6503, 0.7367, 1.0233, 1.6512,\n",
      "         0.5259, 0.0636],\n",
      "        [0.1745, 0.2147, 0.3821, 0.8402, 0.1906, 0.6002, 0.3752, 0.6744, 0.1280,\n",
      "         0.1355, 0.9050, 1.4088, 0.2229, 0.7396, 1.6503, 0.7367, 1.0233, 1.6512,\n",
      "         0.5259, 0.0636]]) torch.Size([5, 20])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([5, 20])) that is different to the input size (torch.Size([5, 20, 100, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(train_data, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# test = next(iter(dataloader))\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [25]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(train_loader, learning_rate, num_epoch, input_size)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# forward\u001B[39;00m\n\u001B[1;32m     20\u001B[0m output \u001B[38;5;241m=\u001B[39m model(sample)\n\u001B[0;32m---> 21\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# backward\u001B[39;00m\n\u001B[1;32m     24\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:613\u001B[0m, in \u001B[0;36mBCELoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m    612\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 613\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_cross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:3074\u001B[0m, in \u001B[0;36mbinary_cross_entropy\u001B[0;34m(input, target, weight, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3072\u001B[0m     reduction_enum \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction)\n\u001B[1;32m   3073\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize():\n\u001B[0;32m-> 3074\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   3075\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a target size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) that is different to the input size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) is deprecated. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3076\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure they have the same size.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(target\u001B[38;5;241m.\u001B[39msize(), \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m   3077\u001B[0m     )\n\u001B[1;32m   3079\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3080\u001B[0m     new_size \u001B[38;5;241m=\u001B[39m _infer_size(target\u001B[38;5;241m.\u001B[39msize(), weight\u001B[38;5;241m.\u001B[39msize())\n",
      "\u001B[0;31mValueError\u001B[0m: Using a target size (torch.Size([5, 20])) that is different to the input size (torch.Size([5, 20, 100, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    input_size = 100\n",
    "    train_data = NumbersDataset(20, input_size)\n",
    "    '''\n",
    "    Reshape image to (1,20*100*5) and update NN to be able to compare to single val\n",
    "    Maybe not exacly that dim but something that reduces to a single val\n",
    "    '''\n",
    "    dataloader = DataLoader(train_data, batch_size=5, shuffle=True)\n",
    "    # test = next(iter(dataloader))\n",
    "    train(dataloader, 0.5, 20, input_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}