{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-gen notebook\n",
    "Denne notebook er skabt til at holde styr på hvilke metoder Anders har brug til at generer data, og datilhørende kilder\n",
    "\n",
    "### Indhold:\n",
    "1. Simpel \"<i>normal af normal</i>\"\n",
    "2. AR(p) processer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal as mn\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import statsmodels as sm\n",
    "from scipy.interpolate import interp1d\n",
    "from statsmodels.tsa.api import VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simpel \"normal af normal\" data-generering\n",
    "Baseret på side 51 af <b>Statistical inference of informational networks</b>.\n",
    "\n",
    "Vi generer data baseret på\n",
    "$$ X\\overset{i.i.d.}{\\sim}\\mathcal{N}\\left( 0, \\sigma^2_x I \\right) \\\\\n",
    " Y\\overset{i.i.d.}{\\sim}\\mathcal{N}\\left( X, \\sigma^2_y I \\right) $$\n",
    "\n",
    " Så er MI givet ved\n",
    " $$ I(X;\\ Y) = \\log_2\\left( 1 + \\dfrac{\\sigma^2_x}{\\sigma^2_y} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_list(n, var_x, var_y, dim=2):\n",
    "    \"\"\"\n",
    "    Generates a set of random points according to <statistical inference of information in networks>\n",
    "    on page 51. The third variable, Z, is ignored as this is only used to create training data\n",
    "    for MI, not CMI.\n",
    "\n",
    "    The covariance matrices are both a scaled identity matrix.\n",
    "\n",
    "    :param n: Number of points realized\n",
    "    :param var_x: Variance of X\n",
    "    :param var_y: Variance of Y\n",
    "    :param dim: Dimension of the variables X and Y\n",
    "    :return:\n",
    "    :returns values: a realization of the process described in the book. Dimensions are [n, X/Y, dim].\n",
    "                     e.g. [20, 0, 2] gives the second element the 20th realization of X.\n",
    "    :returns mi: The mutual information: I(X; Y)\n",
    "    \"\"\"\n",
    "    values = np.zeros((n, 2, dim))  # init\n",
    "    values[:, 0, :] = mn.rvs(mean=np.zeros(dim), cov=var_x * np.identity(dim), size=(n, dim)).reshape((n, dim))\n",
    "\n",
    "    for i, x in enumerate(values[:, 0, :]):\n",
    "        values[i, 1, :] = mn.rvs(mean=x, cov=var_y * np.identity(dim), size=1)\n",
    "\n",
    "    mi = dim * np.log2(1 + var_x / var_y) / 2\n",
    "\n",
    "    return values, mi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AR(p) processer\n",
    "En AR(p) er givet ved\n",
    "$$ x_k = \\omega_k + \\sum^{p}_{i=1}A_ix_{k-1}, \\quad \\omega_k \\overset{i.i.d.}{\\sim} \\mathcal{N}\\left( 0, \\Sigma_\\omega \\right) $$\n",
    "\n",
    "Det er åbenlyst at $E[x_k]= 0 + E[x_0]$. Hvis vi definerer $Var[x_k]=\\sigma_k^2$, og $A_i(x)=A_ixA_i^\\top$ så kan vi se at\n",
    "$\n",
    "\\begin{align}\n",
    "Var[x_0] &= \\sigma_0^2 \\\\\n",
    "Var[x_1] &= A_1(\\sigma_0) + \\Sigma_\\omega \\\\\n",
    "Var[x_2] &= A_1(\\sigma_1) + A_2(\\sigma_0) + \\Sigma_\\omega \\\\\n",
    "&\\ \\ \\vdots \\\\\n",
    "Var[x_k] &= \\Sigma_\\omega + \\sum^{p}_{i=1} A_i(\\sigma_{k-i})\n",
    "\\end{align}\n",
    "$\n",
    "Dette udtryk kan udregnes løbende når vi simulerer vores AR(p) process. <i>NOTE: Der findes nogle ligninger som viser hvad variansen går imod, tror jeg. Variansen konvergerer nemlig meget hurtigt i praksis</i>\n",
    "\n",
    "Vi kan også nemt argumenterer for at $x_k$ er fordelt efter multivariat gaussisk fordeling (da den bare er en lin.komb. af $\\omega_i$)\n",
    "\n",
    "Marginalen for et subset af $x$, $x_s$, har fordelingen:\n",
    "$$ x_s \\sim \\mathcal{N}\\left( S\\mu,S\\Sigma S^\\top \\right) $$\n",
    "hvis $\\mu$ og $\\Sigma$ er mean og varians for $x$. Her er $S$ defineret som $s_{ij}=1$ hvis det j'te element i $x_s$ er det i'te element i $x$\n",
    "\n",
    "Hvis vi vil undersøge MI indgangsvist kan vi tage udgangspunkt i [denne](https://www.math.nyu.edu/~kleeman/infolect7.pdf) lecture note. Det skal dog siges at det er nok noget vi selv kan udlede, eller finde en mere fast kilde på senere. På side 2 i kilden får vi givet at\n",
    "$$ I(X;Y)=\\dfrac{1}{2}log_2\\left( \\dfrac{|\\Sigma_X| |\\Sigma_Y|}{|\\Sigma|} \\right)$$\n",
    "Hvor $\\Sigma$ er covariancen for den jointe fordeling imellem $X$ og $Y$.\n",
    "\n",
    "I praksis vil det sige at hvis vi tagetr $x_k^{(i)}$ til at betyde den i'te indgang i $x$ til index $k$, så får vi \n",
    "$\\begin{align}\n",
    "I(X_k^{(i)}; X_k^{(j)}) &= \\dfrac{1}{2}log_2\\left(\\dfrac{ det\\left(\\Sigma_{ii}\\right) det\\left( \\Sigma_{jj}\\right)}{ det\\left( \\begin{bmatrix} \\Sigma_{ii} & \\Sigma_{ij} \\\\ \\Sigma_{ji} & \\Sigma_{jj} \\end{bmatrix}\\right) }\\right) \\\\\n",
    "&= \\dfrac{1}{2}log_2\\left(\\dfrac{ \\Sigma_{ii}\\Sigma_{jj}}{\\Sigma_{ii}\\Sigma_{jj} -2\\Sigma_{ij}}\\right)\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MI(idx1, idx2, cov):\n",
    "    \"\"\"Calculates the mutual information between the marginals of a multivariate gaussian distribution: I(X1; X2)\n",
    "\n",
    "    Args:\n",
    "        idx1 (int): Index of X1 in the vector it came from\n",
    "        idx2 (int): Index of X2 in the vector it came from\n",
    "        cov (2D array float): Covariance matrix of the vector X, which X1 and X2 came from\n",
    "\n",
    "    Returns:\n",
    "        float: Returns the mutual information of X1 and X2\n",
    "    \"\"\"\n",
    "    top = cov[idx1, idx1] * cov[idx2, idx2]\n",
    "    bot = top - 2*cov[idx1, idx2]\n",
    "\n",
    "    return .5 * np.log2(top / bot)\n",
    "\n",
    "\n",
    "def simulate_AR(coefficients, noise_cov, num_steps, x0=None):\n",
    "    \"\"\"Simulate the AR(p) process, with iid zero-mean gaussian additive noise.\n",
    "\n",
    "    Args:\n",
    "        coefficients (list of arrays): List of the coefficient matrices to use in the realization. Entries in list are 2D float arrays\n",
    "        noise_cov (2D array float): Covariance matrix of the gaussian additive noise\n",
    "        num_steps (int): Number of iterations for which the simulation should run\n",
    "        x0 (1D float array, optional): The starting value of the process. If specified the derivations of the MI don't fit as of the end of september 2022. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        array float: Returns the resulting array of the realization of the process\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    p = len(coefficients)\n",
    "    dim = noise_cov.shape[0]\n",
    "    results = np.zeros((num_steps + 1, dim))\n",
    "\n",
    "    # initialize the initial value\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros(p)\n",
    "    results[0] = x0\n",
    "\n",
    "    # simulate the process\n",
    "    for i in range(1, num_steps + 1):\n",
    "        iter_sum = np.zeros(dim)\n",
    "\n",
    "        # Try except makes sure we don't try to index results too far back. Could be solved with a bool check changed when i >= p\n",
    "        for j in range(p):\n",
    "            try:\n",
    "                iter_sum += coefficients[j] @ results[i - (p + 1)]\n",
    "            except IndexError:\n",
    "                break\n",
    "        \n",
    "        # Save iteration to results\n",
    "        iter_sum += np.random.multivariate_normal(mean=np.zeros(dim), cov=noise_cov)\n",
    "        results[i] = iter_sum\n",
    "    \n",
    "    return results\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kode til at hente data fra nettet\n",
    "Koden hernede henter data fra FRED. Det er en blanding af lidt økonomisk tidsserie data. Funktionen `get_fred_data()` bruger `pandas-datareader`til at hente det ned.\n",
    "\n",
    "Funktionen `interpolate_data` tager den data liste der er genereret af `get_fred_data()` og interpolerer det, sådan at vi har et lieg antal punkter for hvert data-sæt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fred_data():\n",
    "    \"\"\"Pulls data from FRED\n",
    "\n",
    "    Returns:\n",
    "        List of arrays: A list of time-series data pulled from the FRED website.\n",
    "    \"\"\"\n",
    "    # Info to get data from fred\n",
    "    data_names = [\n",
    "            \"PCOPPUSDM\",\n",
    "            \"PWHEAMTUSDM\",\n",
    "            \"PRAWMINDEXM\",\n",
    "            \"APU000072610\",\n",
    "            \"APU0000703112\",\n",
    "            \"DHHNGSP\",\n",
    "            \"PSUNOUSDM\",\n",
    "            \"WTISPLC\",\n",
    "            \"DCOILWTICO\",\n",
    "            \"DCOILBRENTEU\",\n",
    "            \"PNGASEUUSDM\",\n",
    "            \"APU0000708111\",\n",
    "        ]\n",
    "    dates = [\"1990-01-01\", \"2020-01-01\"]\n",
    "    data_list = []\n",
    "\n",
    "    # load the data from the website\n",
    "    for i in range(len(data_names)):\n",
    "        name = data_names[i]\n",
    "        _dat = pdr.get_data_fred(name, dates[0], dates[1])\n",
    "        _dat = _dat.interpolate(method='nearest').ffill().bfill()  # interpolate to remove NaN values\n",
    "        data_list.append(_dat)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    arr_dat = []\n",
    "    for dat in data_list:\n",
    "        arr_dat.append(dat.to_numpy())\n",
    "    \n",
    "    return arr_dat\n",
    "\n",
    "\n",
    "def interpolate_data(data_list, recalled=False):\n",
    "    \"\"\"Interpolates the data loaded by the function get_fred_data(). Should be able to be generalized if need be.\n",
    "\n",
    "    Args:\n",
    "        data_list (List of arrays): List returned by get_fred_data()\n",
    "        recalled (bool, optional): DO NOT CHANGE MANUALLY. This boolean was used when developing the function. If set to true no second run will be done when trying to interpolate. Defaults to False.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Raises an exception if NaN is present in the interpolated data. If that is the case Anders will jump in front of a bus.\n",
    "\n",
    "    Returns:\n",
    "        List of arrays: List of interpolated time-series data from get_fred_data()\n",
    "    \"\"\"\n",
    "    max_data_len = 0\n",
    "    min_data_len = np.Inf\n",
    "\n",
    "    # get longest length of data\n",
    "    for dat in data_list:\n",
    "        max_data_len = max(max_data_len, len(dat))\n",
    "        min_data_len = min(min_data_len, len(dat))\n",
    "    \n",
    "    # we interpolate from 0 to 10e4 such that we have an axis, and dont need much decimal precision\n",
    "    interp_axis = np.linspace(1, 10e4-1, max_data_len)\n",
    "    interpolated_data = []\n",
    "\n",
    "    for dat in data_list:\n",
    "        n = len(dat)\n",
    "        if n != max_data_len:\n",
    "            _axis = np.linspace(0, 10e4, n)\n",
    "            f = interp1d(_axis, dat.reshape(len(dat)), kind='cubic', fill_value=np.mean(dat), bounds_error=False)\n",
    "            inter_data = f(interp_axis)\n",
    "        \n",
    "        else:\n",
    "            inter_data = dat\n",
    "        \n",
    "        interpolated_data.append(inter_data.reshape(len(inter_data)))\n",
    "\n",
    "    # Scipy is kinda weird, so we check if there are NaN values in the interpolated data\n",
    "    has_nan = []\n",
    "    \n",
    "    # check each array if it contains nan\n",
    "    for inter in interpolated_data:\n",
    "        has_nan.append(np.isnan(np.sum(inter)))\n",
    "    \n",
    "    if True not in has_nan:  # everything went according to plan, we return the data\n",
    "        return interpolated_data\n",
    "    \n",
    "    if not recalled:  # something went wrong in the first case, we try again\n",
    "        interpolate_data(data_list, recalled=True)\n",
    "    \n",
    "    # We tried twice, and didn't get the data to interpolate\n",
    "    raise Exception(\"Data would not interpolate in two tries in function interpolate_data()\")\n",
    "\n",
    "    return interpolated_data\n",
    "\n",
    "\n",
    "def create_coefficients(data_list, series_list, model_order):\n",
    "    \"\"\"Creates the coefficients needed to simulate the data, using statsmodels package. \n",
    "\n",
    "    Args:\n",
    "        data_list (List of arrays): List of time-series data returned by the interpolation function.\n",
    "        series_list (List of int): Indexes (0, ..., 11) which decides what data to use in the model. Len(series_list) = k -> model of dimension k\n",
    "        model_order (Int): Order of the model. Must be int >= 1\n",
    "\n",
    "    Returns:\n",
    "        Tuple (array, array): Results from VAR.fit(model_order)\n",
    "            array: coefficient matrices. Size is [p, k, k] where p is the model order, and k is the dimension of the model\n",
    "            array: covariance matrix. Size is [k, k]\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    for idx in series_list:\n",
    "        new_data.append(data_list[idx])\n",
    "    \n",
    "    exog_var = np.array(new_data)\n",
    "\n",
    "    model = VAR(exog_var.T)\n",
    "    result = model.fit(model_order)\n",
    "\n",
    "    return result.coefs, result.sigma_u\n",
    "\n",
    "\n",
    "def save_load_coefs(name, coefficients=None, covariance=None, save=False):\n",
    "    \"\"\"Save or Load coeffiecients from the AR(p) model\n",
    "\n",
    "    Args:\n",
    "        name (str): name under which the coefficients should be save. using name='abc' saves or loads files 'abc_coefs.npy' and 'abc_cov.npy'\n",
    "        coefficients (array): array of the coefficients returned from create_coefficients()\n",
    "        covariance (array): array of the covariance returned from create_coefficients()\n",
    "        save (bool, optional): If set to True the function saves the data specified in coefficients and covariance. If False the functions loads files specified by name. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Tuple (array, array | None): If save is false the function returns None\n",
    "            array: Coefficient matrix loaded by name\n",
    "            array: Covariance matrix loaded by name\n",
    "    \"\"\"\n",
    "    coef_str = name + \"_coefs.npy\"\n",
    "    cov_str = name + \"_cov.npy\"\n",
    "\n",
    "    if save:\n",
    "        np.save(coefficients, coef_str)\n",
    "        np.save(covariance, cov_str)\n",
    "\n",
    "        return None\n",
    "    \n",
    "    coefs = np.load(coef_str)\n",
    "    cov = np.load(cov_str)\n",
    "\n",
    "    return coefs, cov"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('uni_python')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5a7acdd9325f5c11fd27de3f06d5ee506dbe72713cd74a04fd96fc5018595a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
