{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 03 - Gradient Calculation With Autograd</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.0709,  3.4505, -0.3221])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "# print(x)\n",
    "\n",
    "y = x + 2\n",
    "# print(y)\n",
    "\n",
    "z = y * y * 2\n",
    "z = z.mean()\n",
    "# print(z)\n",
    "\n",
    "z.backward() # dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .backward will create a vector-jacobian product in the backgraound (J*v = x.grad). This means that .backward is\n",
    "okay with no input if z is a scalar, but it needs a vector as input if z is not, example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.6096, 13.3944,  0.1167], grad_fn=<MulBackward0>)\n",
      "tensor([ 3.9922, 13.8021, -0.3231])\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 2\n",
    "print(z)\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "z.backward(v) # if z is a \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2734,  0.4469,  0.5170], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "# Tree ways to prevent tracking the gradiant.\n",
    "# x.requires_grad_(False)\n",
    "# print(x)\n",
    "# y = x.detach()\n",
    "# print(y)\n",
    "# with torch.no_grad():\n",
    "#   y = x + 2\n",
    "#   print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "  model_output = (weights*3).sum()\n",
    "  model_output.backward()\n",
    "  print(weights.grad)\n",
    "\n",
    "  weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m4\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      4\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/sgd.py:105\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m nesterov \u001b[39mand\u001b[39;00m (momentum \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m dampening \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m    104\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m \u001b[39msuper\u001b[39;49m(SGD, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(params, defaults)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:40\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_for_profile()\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(params, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mparams argument given to the optimizer should be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39man iterable of Tensors or dicts, but got \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m     42\u001b[0m                     torch\u001b[39m.\u001b[39mtypename(params))\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m defaultdict(\u001b[39mdict\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "optimizer = torch.optim.SGD( weights, lr=0.01 )\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 04 - Backpropagation - Theory With Example</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass and compute the loss\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n",
    "print(loss)\n",
    "\n",
    "# backwards pass\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "### update weights \n",
    "### next forward and backward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 05 - Gradient Descent with Autograd and Backpropagation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before traning: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050331\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "Prediction after traning: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "\n",
    "# f = 2 * x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "  return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "  return ((y_predicted - y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dMSE/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x, y, y_predicted):\n",
    "  return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "print(f'Prediction before traning: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Traning\n",
    "learning_rate = 0.01\n",
    "n_inters = 20\n",
    "\n",
    "for epoch in range(n_inters):\n",
    "  # prediction = forward pass\n",
    "  y_pred = forward(X)\n",
    "\n",
    "  # loss\n",
    "  l = loss(Y, y_pred)\n",
    "\n",
    "  # gradiants\n",
    "  dw = gradient(X, Y, y_pred)\n",
    "\n",
    "  # update weights\n",
    "  w -= learning_rate * dw\n",
    "\n",
    "  if epoch % 2 == 0:\n",
    "    print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after traning: f(5) = {forward(5):.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before traning: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after traning: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "\n",
    "# f = 2 * x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "  return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "  return ((y_predicted - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before traning: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Traning\n",
    "learning_rate = 0.01\n",
    "n_inters = 100\n",
    "\n",
    "for epoch in range(n_inters):\n",
    "  # prediction = forward pass\n",
    "  y_pred = forward(X)\n",
    "\n",
    "  # loss\n",
    "  l = loss(Y, y_pred)\n",
    "\n",
    "  # gradiants = backward pass\n",
    "  l.backward() # dl/dw\n",
    "\n",
    "  # update weights\n",
    "  with torch.no_grad():\n",
    "    w -= learning_rate * w.grad\n",
    "\n",
    "  # zero gradiants\n",
    "  w.grad.zero_()\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after traning: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward is not as exact as the numeriacal gradiant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 06 - Training Pipeline: Model, Loss, and Optimizer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before traning: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after traning: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "\n",
    "# f = 2 * x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "  return w * x\n",
    "\n",
    "print(f'Prediction before traning: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Traning\n",
    "learning_rate = 0.01\n",
    "n_inters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate) #SGD = stocastic gradiant decent\n",
    "\n",
    "for epoch in range(n_inters):\n",
    "  # prediction = forward pass\n",
    "  y_pred = forward(X)\n",
    "\n",
    "  # loss\n",
    "  l = loss(Y, y_pred)\n",
    "\n",
    "  # gradiants = backward pass\n",
    "  l.backward() # dl/dw\n",
    "\n",
    "  # update weights\n",
    "  optimizer.step()\n",
    "\n",
    "  # zero gradiants\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after traning: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before traning: f(5) = 4.327\n",
      "epoch 1: w = 1.058, loss = 9.88688183\n",
      "epoch 11: w = 1.780, loss = 0.26534331\n",
      "epoch 21: w = 1.898, loss = 0.01585601\n",
      "epoch 31: w = 1.919, loss = 0.00887779\n",
      "epoch 41: w = 1.924, loss = 0.00820440\n",
      "epoch 51: w = 1.927, loss = 0.00772281\n",
      "epoch 61: w = 1.929, loss = 0.00727321\n",
      "epoch 71: w = 1.931, loss = 0.00684985\n",
      "epoch 81: w = 1.933, loss = 0.00645116\n",
      "epoch 91: w = 1.935, loss = 0.00607566\n",
      "Prediction after traning: f(5) = 9.870\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "\n",
    "# f = 2 * x\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "out_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, out_size)\n",
    "\n",
    "# Costum model\n",
    "class LinearRegrassion(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super(LinearRegrassion, self).__init__()\n",
    "    # define layers\n",
    "    self.lin = nn.Linear(input_dim, output_dim)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.lin(x)\n",
    "\n",
    "# model = LinearRegrassion(input_size, out_size)\n",
    "\n",
    "print(f'Prediction before traning: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# Traning\n",
    "learning_rate = 0.01\n",
    "n_inters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #SGD = stocastic gradiant decent\n",
    "\n",
    "for epoch in range(n_inters):\n",
    "  # prediction = forward pass\n",
    "  y_pred = model(X)\n",
    "\n",
    "  # loss\n",
    "  l = loss(Y, y_pred)\n",
    "\n",
    "  # gradiants = backward pass\n",
    "  l.backward() # dl/dw\n",
    "\n",
    "  # update weights\n",
    "  optimizer.step()\n",
    "\n",
    "  # zero gradiants\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    [w, b] = model.parameters()\n",
    "    print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after traning: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 07 - Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 0) prepare data\n",
    "x_np, y_np = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "x = torch.from_numpy(x_np.astype(np.float32))\n",
    "y = torch.from_numpy(y_np.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "# 1) model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2) loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "  # forward pass and loss\n",
    "  y_pred = model(x)\n",
    "  loss = criterion(y_pred, y)\n",
    "\n",
    "  # backward pass\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  optimizer.step()\n",
    "\n",
    "  # empty gradients\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "if epoch+1 % 10 == 0:\n",
    "  print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# plot\n",
    "predicted = model(x).detach().numpy()\n",
    "plt.plot(x_np, y_np, 'ro')\n",
    "plt.plot(x_np, predicted, 'b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 08 - Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss = 0.7603\n",
      "epoch: 11, loss = 0.5801\n",
      "epoch: 21, loss = 0.4768\n",
      "epoch: 31, loss = 0.4114\n",
      "epoch: 41, loss = 0.3664\n",
      "epoch: 51, loss = 0.3333\n",
      "epoch: 61, loss = 0.3079\n",
      "epoch: 71, loss = 0.2876\n",
      "epoch: 81, loss = 0.2709\n",
      "epoch: 91, loss = 0.2570\n",
      "accuracy = 0.8947\n"
     ]
    }
   ],
   "source": [
    "# 0) prepare data\n",
    "bc = datasets.load_breast_cancer()\n",
    "x, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# 1) model\n",
    "# f = wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module):\n",
    "  def __init__(self, n_input_features):\n",
    "    super(LogisticRegression, self).__init__()\n",
    "    self.linear = nn.Linear(n_input_features, 1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    y_pred = torch.sigmoid(self.linear(x))\n",
    "    return y_pred\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "\n",
    "# 2) loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss() # Binary cross entropy loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "  # forward pass and loss\n",
    "  y_pred = model(x_train)\n",
    "  loss = criterion(y_pred, y_train)\n",
    "\n",
    "  # backward pass\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  optimizer.step()\n",
    "\n",
    "  # empty gradients\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    print(f'epoch: {epoch}, loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "  y_pred = model(x_test)\n",
    "  y_pred_cls = y_pred.round()\n",
    "  acc = y_pred_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "  print(f'accuracy = {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 09 - Dataset and DataLoader - Batch Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n",
      "epoch 1/2, step 0/45, input torch.Size([4, 13])\n",
      "epoch 1/2, step 5/45, input torch.Size([4, 13])\n",
      "epoch 1/2, step 10/45, input torch.Size([4, 13])\n",
      "epoch 1/2, step 15/45, input torch.Size([4, 13])\n",
      "epoch 1/2, step 20/45, input torch.Size([4, 13])\n",
      "epoch 1/2, step 25/45, input torch.Size([4, 13])\n",
      "epoch 1/2, step 30/45, input torch.Size([4, 13])\n",
      "epoch 1/2, step 35/45, input torch.Size([4, 13])\n",
      "epoch 1/2, step 40/45, input torch.Size([4, 13])\n",
      "epoch 2/2, step 0/45, input torch.Size([4, 13])\n",
      "epoch 2/2, step 5/45, input torch.Size([4, 13])\n",
      "epoch 2/2, step 10/45, input torch.Size([4, 13])\n",
      "epoch 2/2, step 15/45, input torch.Size([4, 13])\n",
      "epoch 2/2, step 20/45, input torch.Size([4, 13])\n",
      "epoch 2/2, step 25/45, input torch.Size([4, 13])\n",
      "epoch 2/2, step 30/45, input torch.Size([4, 13])\n",
      "epoch 2/2, step 35/45, input torch.Size([4, 13])\n",
      "epoch 2/2, step 40/45, input torch.Size([4, 13])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class WineDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize data, download, etc.\n",
    "        # read with numpy or pandas\n",
    "        xy = np.loadtxt('./data/wineData.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "        # here the first column is the class label, the rest are the features\n",
    "        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]\n",
    "        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "# create dataset\n",
    "dataset = WineDataset()\n",
    "\n",
    "# get first sample and unpack\n",
    "# first_data = dataset[0]\n",
    "# features, labels = first_data\n",
    "# print(features, labels)\n",
    "dataLoader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2) # num_workers = makes loading falser, is uses multiple subprocesses \n",
    "\n",
    "# dataiter = iter(dataLoader)\n",
    "# data = dataiter.next()\n",
    "# features, labels = data\n",
    "# print(features, labels)\n",
    "\n",
    "# traning loop\n",
    "num_epoch = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "  for i, (input, labels) in enumerate(dataLoader):\n",
    "    # dorward backward, update\n",
    "    if i %5 == 0:\n",
    "      print(f'epoch {epoch+1}/{num_epoch}, step {i}/{n_iterations}, input {input.shape}')\n",
    "\n",
    "# torchvision.datasets.MNIST()\n",
    "# fasion-mnist, cifar, coco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 10 - Dataset Transforms</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "tensor([[5.1080e+01, 1.3720e+01, 7.9200e+00, 6.4000e+01, 3.2000e+02, 6.5200e+00,\n",
      "         5.0000e+00, 1.7200e+00, 3.3200e+00, 1.3600e+01, 2.8000e+00, 8.4800e+00,\n",
      "         1.4880e+03],\n",
      "        [4.6480e+01, 7.9600e+00, 9.1200e+00, 7.2000e+01, 3.9200e+02, 1.2080e+01,\n",
      "         9.0400e+00, 6.8000e-01, 5.4000e+00, 1.3000e+01, 4.6400e+00, 1.1840e+01,\n",
      "         1.3800e+03]])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# dataset = torchvision.datasets.MNIST( root='./data', transform=torchvision.transforms.ToTenser() )\n",
    "class WineDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        # Initialize data, download, etc.\n",
    "        # read with numpy or pandas\n",
    "        xy = np.loadtxt('./data/wineData.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "        # here the first column is the class label, the rest are the features\n",
    "        self.x_data = xy[:, 1:] # size [n_samples, n_features]\n",
    "        self.y_data = xy[:, [0]] # size [n_samples, 1]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x_data[index], self.y_data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        inputs, targest = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targest)\n",
    "\n",
    "class MulTransform():\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        inputs, target = sample\n",
    "        inputs = input * self.factor\n",
    "        return inputs, target\n",
    "\n",
    "# create dataset\n",
    "dataset = WineDataset(transform=ToTensor())\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels))\n",
    "\n",
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(4)])\n",
    "dataset = WineDataset(transform=composed)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 11 - Softmax and Cross Entropy</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Softmax</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65900114 0.24243297 0.09856589]\n",
      "tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "x = np.array([2, 1, 0.1])\n",
    "outputs = softmax(x)\n",
    "print(outputs)\n",
    "\n",
    "x = torch.tensor([2, 1, 0.1])\n",
    "outputs = torch.softmax(x, dim=0)\n",
    "print(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross Entropy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 numpy: 0.3567\n",
      "Loss2 numpy: 2.3026\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(actual, predicted):\n",
    "    EPS = 1e-15\n",
    "    predicted = np.clip(predicted, EPS, 1 - EPS)\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss # / float(predicted.shape[0])\n",
    "\n",
    "# y must be one hot encoded\n",
    "# if class 0: [1 0 0]\n",
    "# if class 1: [0 1 0]\n",
    "# if class 2: [0 0 1]\n",
    "Y = np.array([1, 0, 0])\n",
    "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "l1 = cross_entropy(Y, Y_pred_good)\n",
    "l2 = cross_entropy(Y, Y_pred_bad)\n",
    "print(f'Loss1 numpy: {l1:.4f}')\n",
    "print(f'Loss2 numpy: {l2:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2991693317890167\n",
      "1.6241613626480103\n",
      "tensor([2, 0, 1])\n",
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3 samples\n",
    "y = torch.tensor([2, 0, 1])\n",
    "\n",
    "# size = n_samples * n_classes = 3*3\n",
    "y_pred_good = torch.tensor([[0.01, 1.0, 2.1], [2.0, 1.0, 0.1], [0.1, 3.0, 0.1]])\n",
    "y_pred_bad = torch.tensor([[2.1, 1.0, 0.1], [0.1, 1.0, 2.1], [0.1, 3.0, 0.1]])\n",
    "\n",
    "l1 = loss(y_pred_good, y)\n",
    "l2 = loss(y_pred_bad, y)\n",
    "print(l1.item())\n",
    "print(l2.item())\n",
    "\n",
    "_, prediction1 = torch.max(y_pred_good, 1)\n",
    "_, prediction2 = torch.max(y_pred_bad, 1)\n",
    "print(prediction1)\n",
    "print(prediction2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Simple Nerual Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Multiclass problem\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()  # (applies Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 12 - Activation Functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1 (create nn modules)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# option 2 (use activation functions directly in forward pass)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 13 - Feed-Forward Neural Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "81.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m n_total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[0;32m---> 49\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i, (img, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# 100, 1, 28, 28\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# 100, 784\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     53\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    127\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:135\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_tensor\u001b[39m(pic) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    124\u001b[0m     \u001b[39m\"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39m    This function does not support torchscript.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mis_tracing():\n\u001b[1;32m    136\u001b[0m         _log_api_usage_once(to_tensor)\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (F_pil\u001b[39m.\u001b[39m_is_pil_image(pic) \u001b[39mor\u001b[39;00m _is_numpy(pic)):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/jit/_trace.py:1015\u001b[0m, in \u001b[0;36mis_tracing\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[39mif\u001b[39;00m is_scripting():\n\u001b[1;32m   1014\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1015\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_is_tracing()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cpu') #torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# hyper parameters\n",
    "input_size = 784 # 28x28 pic\n",
    "hidden_size = 100\n",
    "num_classes = 10 # digtest 0 to 9\n",
    "num_epoch = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "#MNIST\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "for i in range(6):\n",
    "  plt.subplot(2, 3, i+1)\n",
    "  plt.imshow(samples[i][0], cmap='gray')\n",
    "# plt.show()\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_classes):\n",
    "    super(NeuralNet, self).__init__()\n",
    "    self.l1 = nn.Linear(input_size, hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = self.l1(x)\n",
    "    out = self.relu(out)\n",
    "    out = self.l2(out)\n",
    "    return out\n",
    "  \n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epoch):\n",
    "  for i, (img, labels) in enumerate(train_loader):\n",
    "    # 100, 1, 28, 28\n",
    "    # 100, 784\n",
    "    img = img.reshape(-1, 28*28).to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    #forward\n",
    "    output = model(img)\n",
    "    loss = criterion(output, labels)\n",
    "\n",
    "    #backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i) % 100 == 0:\n",
    "      print(f'epoch {epoch} / {num_epoch}, step {i}/{n_total_steps} loss = {loss.item():.4f}')\n",
    "\n",
    "# test\n",
    "with torch.no_grad():\n",
    "  n_correct = 0\n",
    "  n_samples = 0\n",
    "  for img, labels in test_loader:\n",
    "    img = img.reshape(-1, 28*28).to(device)\n",
    "    labels = labels.to(device)\n",
    "    output = model(img)\n",
    "\n",
    "    # value, index (index er class label)\n",
    "    _, predictions = torch.max(output, 1)\n",
    "    n_samples += labels.shape[0]\n",
    "    n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "  acc = 100.0 * n_correct / n_samples\n",
    "  print(f'acc = {acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Tutorial 14 - Convolutional Neural Network (CNN)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Epoch [1/5], Step [2000/12500], Loss: 2.2849\n",
      "Epoch [1/5], Step [4000/12500], Loss: 2.2677\n",
      "Epoch [1/5], Step [6000/12500], Loss: 2.2341\n",
      "Epoch [1/5], Step [8000/12500], Loss: 2.2668\n",
      "Epoch [1/5], Step [10000/12500], Loss: 2.3231\n",
      "Epoch [1/5], Step [12000/12500], Loss: 1.6277\n",
      "Epoch [2/5], Step [2000/12500], Loss: 2.0229\n",
      "Epoch [2/5], Step [4000/12500], Loss: 1.7986\n",
      "Epoch [2/5], Step [6000/12500], Loss: 1.6878\n",
      "Epoch [2/5], Step [8000/12500], Loss: 1.7350\n",
      "Epoch [2/5], Step [10000/12500], Loss: 1.1807\n",
      "Epoch [2/5], Step [12000/12500], Loss: 1.8516\n",
      "Epoch [3/5], Step [2000/12500], Loss: 1.4325\n",
      "Epoch [3/5], Step [4000/12500], Loss: 1.8096\n",
      "Epoch [3/5], Step [6000/12500], Loss: 1.6112\n",
      "Epoch [3/5], Step [8000/12500], Loss: 1.6113\n",
      "Epoch [3/5], Step [10000/12500], Loss: 1.0544\n",
      "Epoch [3/5], Step [12000/12500], Loss: 1.2439\n",
      "Epoch [4/5], Step [2000/12500], Loss: 1.9567\n",
      "Epoch [4/5], Step [4000/12500], Loss: 1.0605\n",
      "Epoch [4/5], Step [6000/12500], Loss: 1.4898\n",
      "Epoch [4/5], Step [8000/12500], Loss: 1.7458\n",
      "Epoch [4/5], Step [10000/12500], Loss: 1.3725\n",
      "Epoch [4/5], Step [12000/12500], Loss: 1.2958\n",
      "Epoch [5/5], Step [2000/12500], Loss: 0.9305\n",
      "Epoch [5/5], Step [4000/12500], Loss: 0.9055\n",
      "Epoch [5/5], Step [6000/12500], Loss: 1.5920\n",
      "Epoch [5/5], Step [8000/12500], Loss: 1.5293\n",
      "Epoch [5/5], Step [10000/12500], Loss: 1.7428\n",
      "Epoch [5/5], Step [12000/12500], Loss: 0.9818\n",
      "Finished Training\n",
      "Accuracy of the network: 48.4 %\n",
      "Accuracy of plane: 56.4 %\n",
      "Accuracy of car: 69.0 %\n",
      "Accuracy of bird: 20.4 %\n",
      "Accuracy of cat: 10.7 %\n",
      "Accuracy of deer: 38.1 %\n",
      "Accuracy of dog: 49.1 %\n",
      "Accuracy of frog: 70.5 %\n",
      "Accuracy of horse: 60.0 %\n",
      "Accuracy of ship: 57.0 %\n",
      "Accuracy of truck: 52.8 %\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cpu') #torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "# dataset has PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # 3 because the are 3 colorchannels\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
